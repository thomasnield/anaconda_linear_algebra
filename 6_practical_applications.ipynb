{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ffd5a6",
   "metadata": {},
   "source": [
    "# Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f3681",
   "metadata": {},
   "source": [
    "We have learned just enough linear algebra to be dangerous and get comfortable with using machine learning and data science libraries. However, we are going to go a step further and use linear algebra to solve a couple of problems \"from scratch\" using only NumPy. By solving these problems, you will see how linear algebra is used in practice and get a little further insight in how libraries and techniques work. \n",
    "\n",
    "Let's start with solving a system of equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13733d9",
   "metadata": {},
   "source": [
    "## Systems of Equations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b71af6",
   "metadata": {},
   "source": [
    "Let's put inverse matrices to some use. Let's say you are provided a system of linear equations as shown below and you need to solve for $ x $, $ y $, and $ z $. \n",
    "\n",
    "$\n",
    "2x + 9y - 3z = 12 \\\\\n",
    "x + 2y + 7z = 5 \\\\\n",
    "x + 2y + 3z = 6\n",
    "$\n",
    "\n",
    "You could try to solve this algebraically, but you can actually approach this with a linear algebra approach. First, let's extract the coefficients mutliplied on each variable. Note that if there is no coefficient, the coefficient is effectively a $ 1 $ as multiplying by $ 1 $ has no impact. Also, a subtraction instead of an addition of an element will treat the coefficient as negative. \n",
    "\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} \n",
    "2 & 9 & -3 \\\\\n",
    "1 & 2 & 7 \\\\\n",
    "1 & 2 & 3 \n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df155bd9",
   "metadata": {},
   "source": [
    "Before we go any further, let's make sure the determinant of this matrix is not zero. If it is, that means our system of equations will be unsolvable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fede0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import det\n",
    "\n",
    "A = array([\n",
    "    [2, 9, -3],\n",
    "    [1, 2, 7],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "\n",
    "det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b5449",
   "metadata": {},
   "source": [
    "Alright, the determinant is approximately 20 so we are clear to move forward. \n",
    "\n",
    "Next, let's grab the terms on the right side of the equals sign  $ = $ and make that vector $ B $. \n",
    "\n",
    "$\n",
    "B = \\begin{bmatrix} 12 \\\\ 5 \\\\ 6 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Now let's consider a vector $ X $ which contains all three unsolved variables $ x $, $ y $, and $ z $. \n",
    "\n",
    "$ \n",
    "X = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}\n",
    "$\n",
    "\n",
    "If we perform matrix vector multiplication between $ A $ and $ X $, that will result in vector $ B $. \n",
    "\n",
    "$\n",
    "AX = B\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "2 & 9 & -3 \\\\\n",
    "1 & 2 & 7 \\\\\n",
    "1 & 2 & 3 \n",
    "\\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} 12 \\\\ 5 \\\\ 6 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "2x + 9y -3z \\\\\n",
    "x + 2y + 7z \\\\\n",
    "x + 2y + 3z \n",
    "\\end{bmatrix} = \\begin{bmatrix} 12 \\\\ 5 \\\\ 6 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Let's go back to this expression:\n",
    "\n",
    "$\n",
    "AX = B\n",
    "$\n",
    "\n",
    "If we \"multiply\" each side by the inverse of matrix $ A $, which we will denote as $ A^{-1} $, we can effectively isolate the $ X $. \n",
    "\n",
    "$ \n",
    "A^{-1}AX= A^{-1}B\n",
    "$\n",
    "\n",
    "$ \n",
    "X = A^{-1}B\n",
    "$\n",
    "\n",
    "The reason $ A^{-1}A $ cancels out is its matrix multiplication results in an identity matrix, effectively isolating $ x $, $ y $, and $ z $. Mutiplying by an identity matrix is the linear algebra equivalent of multiplying by $ 1 $. It has no effect.\n",
    "\n",
    "$\n",
    "A^{-1}A = \\begin{bmatrix} \n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{bmatrix} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8355c",
   "metadata": {},
   "source": [
    "$\n",
    "A^{-1}AX = \\begin{bmatrix} \n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5bff1",
   "metadata": {},
   "source": [
    "I can prove this using NumPy. Let's calculate the inverse of $ A $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39ee5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import inv\n",
    "\n",
    "A = array([\n",
    "    [2, 9, -3],\n",
    "    [1, 2, 7],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "\n",
    "A_inv = inv(A)\n",
    "\n",
    "A_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad897a",
   "metadata": {},
   "source": [
    "And let's apply the inverse $ A^{-1} $ to $ A $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_inv @ A "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acb3f8",
   "metadata": {},
   "source": [
    "However, because NumPy has some floating point mess as shown above, I prefer to use SymPy as it will give me a much cleaner answer with only 1's and 0's. That's because it does symbolic math rather than floating point arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63c573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sympy import Matrix\n",
    "\n",
    "A = Matrix([\n",
    "    [2, 9, -3],\n",
    "    [1, 2, 7],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "\n",
    "A_inv = A.inv()\n",
    "A_inv @ A "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02103907",
   "metadata": {},
   "source": [
    "Because of this reasoning, we can calculate the $ X $ vector containing $ x $, $ y $, and $ z $ using the inverse of matrix $ A $ multiplied with vector $ B $.\n",
    "\n",
    "$ \n",
    "X = A^{-1}B\n",
    "$\n",
    "\n",
    "Let's use NumPy to now solve this system of equations using that simple expression that solves $ X $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# 2x + 9y - 3z = 12\n",
    "# 1x + 2y + 7z = 5\n",
    "# 1x + 2y + 3z = 6\n",
    "\n",
    "A = array([\n",
    "    [2, 9, -3],\n",
    "    [1, 2, 7],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "\n",
    "B = array([\n",
    "    12,\n",
    "    5,\n",
    "    6\n",
    "])\n",
    "\n",
    "X = inv(A) @ B \n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d5aeb",
   "metadata": {},
   "source": [
    "Therefore, we find that $ X = \\begin{bmatrix} 7.65 \\\\ -0.45 \\\\ -0.25 \\end{bmatrix} $. This means $ x = 7.65 $, $ y = -0.45 $, and $ z = -0.25 $. \n",
    "\n",
    "Solving a system of equations like this extends to many problems like linear programming and many areas of scientific research.\n",
    "\n",
    "Here is a visualization of this system of equations being solved. Notice how the yellow vector $ B $ shifts after the basis vectors reflecting matrix $ A $ are moved to their identity positions. This results in vector $ B $ becoming vector $ X $, effectively solving for the variables. \n",
    "\n",
    "<video src=\"https://github.com/thomasnield/anaconda_linear_algebra/raw/main/media/01_SystemOfEquationsScene.mp4\" controls=\"controls\" style=\"max-width: 730px;\">\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cfc2e",
   "metadata": {},
   "source": [
    "## Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97352c0b",
   "metadata": {},
   "source": [
    "Matrix decomposition is breaking up a matrix into components, much like factoring numbers (e.g., 6 can be factored to 2 Ã— 3). We use matrix decomposition for several tasks like fitting a linear regression (which we will do after this section) and calculating inverse matrices. For this example, we will talk about a common type of matrix decomposition called eigendecomposition, which is used often in machine learning and principal component analysis. At this level we do not have the bandwidth to dive into each of these applications but we will at least learn the process to get familiar with decomposition. \n",
    "\n",
    "The formula for eigendecomposition is as follows, where $ v $ is the eigenvalues and $ \\lambda $ is the eigenvectors. $ A $ is the original matrix. \n",
    "\n",
    "$ \n",
    "Av = \\lambda v\n",
    "$ \n",
    "\n",
    "There is one eigenvector and eigenvalue for each dimension of the matrix $ A $, and not every matrix can be decomposed with eigendecomposition.\n",
    "\n",
    "Let's perform eigendecomposition on matrix $ A $ below by using the `eig()` function in NumPy's `linalg` package. This is going to result in those two components: eigenvectors and eigenvalues. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, diag\n",
    "from numpy.linalg import eig, inv \n",
    "\n",
    "\n",
    "A = array([\n",
    "    [2, 9,],\n",
    "    [1, 2,]\n",
    "])\n",
    "\n",
    "\n",
    "eigenvals, eigenvecs = eig(A)\n",
    "\n",
    "print(\"EIGENVALUES\")\n",
    "print(eigenvals)\n",
    "print(\"\\nEIGENVECTORS\")\n",
    "print(eigenvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c42c5",
   "metadata": {},
   "source": [
    "Now how do we recompose matrix $ A $ from the eigenvectors and eigenvalues? To reconstruct, we need this formula: \n",
    "\n",
    "$\n",
    "A = Q \\Lambda Q^{-1}\n",
    "$\n",
    "\n",
    "$ Q $ is the eigenvectors, $ \\Lambda $ is the eigenvalues in diagonal form, and $ Q^{-1} $ is the inverse matrix of $ Q $.  \n",
    "\n",
    "Let's reconstruct this with NumPy, and sure enough you will see the original matrix $ A $ put back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b83b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = eigenvecs\n",
    "Q_inv = inv(Q)\n",
    "\n",
    "L = diag(eigenvals)\n",
    "A = Q @ L @ Q_inv\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5cc11",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "\n",
    "Here is another place we can use linear algebra for a highly useful problem. A **linear regression** fits a straight line to observed data, attempting to demonstrate a linear relationship between variables and make predictions on new data yet to be observed. While there are many ways to fit a linear regression, including gradient descent, we can use inverse matrices as well as matrix decomposition techniques. Let's start with an inverse matrix technique. \n",
    "\n",
    "Let's first bring in a dataset containing two columns $ x $ and $ y $ from Github, and save it to a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "url = r\"https://raw.githubusercontent.com/thomasnield/machine-learning-demo-data/master/regression/linear_normal.csv\"\n",
    "\n",
    "df = pd.read_csv(url, delimiter=\",\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a780625",
   "metadata": {},
   "source": [
    "Next let's visualize this data using matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aae3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\n",
    "Y = df.values[:, -1]\n",
    "plt.plot(X, Y, 'o') # scatterplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d61188",
   "metadata": {},
   "source": [
    "Looking ath this data there definitely seems to be a linear relationship here, as when $ x $ proportionally increases/decreases then $ y $ proportionally increases/decreases as well. Let's learn how to fit a line using some linear algebra techniques. \n",
    "\n",
    "First, let's observe the following formula to get a vector of coefficients $ b $ for a linear function. \n",
    "\n",
    "$\n",
    "\\Large b = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b08990",
   "metadata": {},
   "source": [
    "$ X $ is a matrix of the input variable values, which in this case only has one column. However, we are going to pad some 1's as an extra column so that will generate an intercept coefficient and not just a slope. $ y $ is the vector of the output variable. $ X^T $ is the transposed matrix of $ X $. \n",
    "\n",
    "Let's first pad $ X $ with an extra column of 1's, and call it `X_1`. We will use this in place of $ X $ in our formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add placeholder \"1\" column to generate intercept\n",
    "X_1 = np.vstack([X.flatten(), np.ones(len(X))]).transpose()\n",
    "\n",
    "X_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929b713",
   "metadata": {},
   "source": [
    "Now let's apply this formula and execute it using NumPy. We will now get the coefficients in vector $ b $. \n",
    "\n",
    "$\n",
    "\\Large b = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = inv(X_1.transpose() @ X_1) @ (X_1.transpose() @ Y)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d82dd",
   "metadata": {},
   "source": [
    "$ 1.75919315 $ is the value for the slope, and $ 4.69359655 $ is the value for the intercept. Let's plot the line through the points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, 'o') # scatterplot\n",
    "plt.plot(X, X_1 @ b) # line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c535a",
   "metadata": {},
   "source": [
    "That looks correct, and for this particular problem this is fine. However, when you have a lot of data with many columns, computers can start to produce unstable results due to floating point precision issues. This is a use case for matrix decomposition, which in this case we can use QR decomposition. By first decomposing $ X $ into the $ Q $ and $ R $ components (and by $ X $ I mean with the column of 1's), we can make this linear regression more numerically stable. \n",
    "\n",
    "We can first decompose $ X $ into components $ Q $ and $ R $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863c840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import qr\n",
    "\n",
    "Q,R = qr(X_1)\n",
    "print(\"Q:\")\n",
    "print(Q)\n",
    "\n",
    "print(\"R:\")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e83f3",
   "metadata": {},
   "source": [
    "Then we can use this formula to calculate the coefficients in vector $ b $. \n",
    "\n",
    "$\n",
    "b = R^{-1} \\cdot Q^T \\cdot y \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09ca4c",
   "metadata": {},
   "source": [
    "All we need from NumPy is some inverse matrix and transposition work, and then some matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf349cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = inv(R) @ Q.transpose() @ Y \n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80744ede",
   "metadata": {},
   "source": [
    "Again, 1.75919315  is the value for the slope, and  4.69359655  is the value for the intercept. This gives us the exact same answer as the plain inverse technique, but for larger and more complex datasets this QR decomposition approach will be more numerically stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60f914",
   "metadata": {},
   "source": [
    "## Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b79e0a",
   "metadata": {},
   "source": [
    "A heavy-duty use case for linear algebra has become neural networks and deep learning. Let's see what this looks like to implement a simple forward propagation neural network completely from scratch using just NumPy. We are not going to learn about backpropagation and gradient descent here, which I [cover in my book _Essential Math for Data Science_](https://learning.oreilly.com/library/view/essential-math-for/9781098102920/). \n",
    "\n",
    "Let's say we have some data representing different background colors (with input variables `RED`, `GREEN`, and `BLUE`). We also have a `LIGHT_OR_DARK_FONT_IND` indicating whether a light (0) or dark (1) font will work best with that background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4122b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://tinyurl.com/y2qmhfsr\")\n",
    "df.sample(10, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e9b55",
   "metadata": {},
   "source": [
    "Let's capture the three input columns as matrix $ X $ and the output column as vector $ Y $. \n",
    "\n",
    "$\n",
    "X = \\begin{bmatrix}179 & 204 & 255\\\\179 & 179 & 179\\\\205 & 102 & 29\\\\173 & 216 & 230\\\\122 & 197 & 205\\\\ & ... & \\\\ 0 & 197 & 205\\\\197 & 193 & 170\\\\70 & 130 & 180\\\\205 & 200 & 177\\\\0 & 139 & 69\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "Y = \\begin{bmatrix}1\\\\1\\\\0\\\\1\\\\1\\\\...\\\\1\\\\1\\\\0\\\\1\\\\0\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a911ca",
   "metadata": {},
   "source": [
    "We can extract the $ X $ and $ Y $ as NumPy arrays from the `DataFrame`. We will also scale $ X $ down by 255 so the input values are between 0 and 1. This will reduce the number space into a smaller, more mathematically convenient range than the full 0-255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:,:3] / 255 \n",
    "Y = df.values[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ce798",
   "metadata": {},
   "source": [
    "Normally, we would use this data to train a neural network to predict a light or dark font for a given background color. However, the neural network we are about to build is already \"trained\" for the sake of scope in this class. \n",
    "\n",
    "We could use simpler models like logistic regression for this problem, but this is a great toy problem to understand neural networks. It is also a microcosm of computer vision, as the three input variables `RED`, `GREEN`, and `BLUE` could represent a single pixel in an image. \n",
    "\n",
    "Let's do a quick crash course on neural networks with the aid of some animation. A neural network is a multi-layered series of multiplication and addition operations (thus, matrix multiplication) with some nonlinear functions in the mix. The output is a value between 0 and 1, suggesting the \"probability\" of a dark font. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dccfb3",
   "metadata": {},
   "source": [
    "\n",
    "<video src=\"https://github.com/thomasnield/anaconda_linear_algebra/raw/main/media/02_NeuralNetworkScene.mp4\" controls=\"controls\" style=\"max-width: 730px;\">\n",
    "</video>\n",
    "\n",
    "Above, we represent the `RED`, `GREEN`, and `BLUE` inputs as $x_1$, $x_2$, and $x_3$. This would represent one row of matrix $ X $. We have three nodes in the *hidden layer* and associate weights $w_1$ through $w_9$ (each between -1 and 1) for each pair of input node to hidden node. We also add a bias value to each node. The resulting value is passed to a nonlinear function called ReLU which simply turns negative values to 0. We repeat this process again between the hidden nodes and the output node, which involves another three weights $w_{10}$ through $w_{12}$ and another bias value $ b_4$. That is passed through a sigmoid function. If that output value is less than $0.5$, we categorize it as `LIGHT`. Otherwise, we categorize it as `DARK`. \n",
    "\n",
    "Note how each layer of nodes can be represented purely by matrices of the weights and biases. Let's capture them below. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "$\n",
    "W_{hidden} = \\begin{bmatrix}\n",
    "w_1 & w_2 & w_3 \\\\\n",
    "w_4 & w_5 & w_6 \\\\\n",
    "w_7 & w_8 & w_9\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "W_{output} = \\begin{bmatrix}\n",
    "w_{10} \\\\\n",
    "w_{11} \\\\\n",
    "w_{12} \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "B_{hidden} = \\begin{bmatrix}b_1\\\\b_2\\\\b_3 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "B_{output} = \\begin{bmatrix}b_4\\end{bmatrix}\n",
    "$ \n",
    "\n",
    "<br><br>\n",
    "\n",
    "Now, normally you would solve for these weight and bias values through a complicated procedure called backpropagation with stochastic gradient descent. For this example, however, I have already solved for these weight and bias values as shown below. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "$\n",
    "W_{hidden} = \\begin{bmatrix}3.5574801792467 & 8.48639024065542 & 1.59453643090894\\\\4.28982009818168 & 8.35518250953765 & 1.36713925567114\\\\3.7207429234428 & 8.13223257221876 & 1.48165938844881\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "W_{output} = \\begin{bmatrix}4.27394193741564 & 3.656340721696 & 2.63047525734278\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "B_{hidden} = \\begin{bmatrix}-6.67311750917892\\\\-6.34084123159501\\\\-6.10933576744567\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "B_{output} = \\begin{bmatrix}-5.46880991264584\\end{bmatrix}\n",
    "$ \n",
    "<br><br>\n",
    "\n",
    "I have declared them in NumPy below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "w_hidden = np.array([\n",
    "    [3.55748018, 8.48639024, 1.59453643],\n",
    "    [4.2898201,  8.35518251, 1.36713926],\n",
    "    [3.72074292, 8.13223257, 1.48165939]\n",
    "])\n",
    "\n",
    "w_output = np.array([\n",
    "    [4.27394194, 3.65634072, 2.63047526]\n",
    "])\n",
    "\n",
    "b_hidden = np.array([\n",
    "    [-6.67311751],\n",
    "    [-6.34084123],\n",
    "    [-6.10933577]\n",
    "])\n",
    "\n",
    "b_output = np.array([\n",
    "    [-5.46880991]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c8c59",
   "metadata": {},
   "source": [
    "Now here is a matrix multiplication and addition operation, with the nonlinear functions `sigmoid` and `relu`, that will take a given input of one or more background colors $ X $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c24431",
   "metadata": {},
   "source": [
    "$ \n",
    "Y_{pred} = \\text{Sigmoid}(W_{output} \\cdot \\text{ReLU}(W_{hidden} \\cdot X + B_{hidden}) + B_{output})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47373d",
   "metadata": {},
   "source": [
    "Let's implement that whole line in NumPy using a `forward_prop()` function. These will output the suggested probabilities of beig a `DARK` font. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160805da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "relu = lambda x: np.maximum(x, 0)\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Runs inputs through the neural network to get predicted outputs\n",
    "def forward_prop(X):\n",
    "    hidden = relu(w_hidden @ X + b_hidden)\n",
    "    output = sigmoid(w_output @ hidden + b_output)\n",
    "    return output\n",
    "\n",
    "# Calculate predictions for all background colors \n",
    "predictions = forward_prop(X.transpose())\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcd143",
   "metadata": {},
   "source": [
    "To better summarize this performance (we will not do best practices like train/test splits or confusion matrices here), we can set all values that are at least $ 0.5 $ to $ 1 $ and anything less to $ 0 $. Then we can compare to the actual $ Y $ values to these new predicted $ Y{pred} $ values. This gives us the means to calculate the total percentage of accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = np.equal((predictions >= .5).flatten().astype(int), Y)\n",
    "accuracy = sum(comparisons.astype(int) / Y.shape[0])\n",
    "print(\"ACCURACY: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d89feb",
   "metadata": {},
   "source": [
    "## Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea347de9",
   "metadata": {},
   "source": [
    "Solve the system of equations below using NumPy. \n",
    "\n",
    "$\n",
    "4x + 1y - 1z = 1 \\\\\n",
    "1x + 0.5y + 2z = 3 \\\\\n",
    "2x + 1y + 2z = -2\n",
    "$\n",
    "\n",
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v \n",
    "\n",
    "The values for $ x $, $y$, and $z$ can be calculated by finding the inverse of the coefficient matrix and multiplying it against the right-hand vector as shown below. \n",
    "\n",
    "The vector $\\begin{bmatrix}7.5 & -25.0 & 4.0\\end{bmatrix} $ contains the three values respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c50b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import sympy as sp \n",
    "\n",
    "# 4x + 1y - z = 1\n",
    "# 1x + 0.5y + 2z = 3\n",
    "# 2x + 1y + 2z = -2\n",
    "\n",
    "A = array([\n",
    "    [4, 1, -1],\n",
    "    [1, 0.5, 2],\n",
    "    [2, 1, 2]\n",
    "])\n",
    "\n",
    "B = array([\n",
    "    1,\n",
    "    3,\n",
    "    -2\n",
    "])\n",
    "\n",
    "X = inv(A) @ B \n",
    "\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
